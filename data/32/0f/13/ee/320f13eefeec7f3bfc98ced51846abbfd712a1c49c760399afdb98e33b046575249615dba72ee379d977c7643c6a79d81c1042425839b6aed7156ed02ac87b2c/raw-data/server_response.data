<!DOCTYPE html><html lang=en> <head><title>SocketIO / EngineIO DoS | callerxyz</title><meta charset=utf-8><meta name=referrer content=origin-when-cross-origin><link href=https://ℬ㏒.㎈ℓℯℛ.ⓧⓨℤ/feeds/all.atom.xml type=application/atom+xml rel=alternate title="callerxyz Full Atom Feed"><meta property=og:site_name content=callerxyz><meta property=og:locale content=en-GB><meta property=og:title content="SocketIO / EngineIO DoS"><meta property=og:description content="Quite a while ago, I reported an application Denial of Service vulnerability in the Socket.IO / Engine.IO parser implementations in nodejs and python. A single HTTP POST request can cause extreme CPU and memory usage, but in nodejs, a single HTTP POST request can even kill the server with a Javascript heap out of memory fatal error."><meta property=og:url content=https://ℬ㏒.㎈ℓℯℛ.ⓧⓨℤ/socketio-engineio-dos><meta property=og:type content=article><meta property=article:published_time content="2020-05-07 23:00:00+01:00"><meta property=article:modified_time content><meta name=keywords content="security, poc, research, nodejs, python"><meta name=tags content=security><meta name=article:tag content=security><meta name=tags content=poc><meta name=article:tag content=poc><meta name=tags content=research><meta name=article:tag content=research><meta name=tags content=nodejs><meta name=article:tag content=nodejs><meta name=tags content=python><meta name=article:tag content=python><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width, initial-scale=1"><link rel=stylesheet href=https://ℬ㏒.㎈ℓℯℛ.ⓧⓨℤ/theme/css/style.css><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css referrerpolicy=no-referrer><link href="https://fonts.googleapis.com/css2?family=Arima+Madurai:wght@400;700&family=Montserrat:ital,wght@0,400;0,800;1,400;1,800&family=Orbitron:wght@700&family=Share+Tech+Mono&display=swap" rel=stylesheet referrerpolicy=no-referrer></head> <body> <main> <article> <header> <h1>SocketIO / EngineIO DoS</h1> <time datetime=2020-05-07T23:00:00+01:00>May 2020</time> </header> <div class=content> <p>Quite a while ago, I reported an application Denial of Service vulnerability in the Socket.IO / Engine.IO parser implementations in nodejs and python. A single HTTP POST request can cause extreme CPU and memory usage, but in nodejs, a single HTTP POST request can even kill the server with a <code>Javascript heap out of memory</code> fatal error.</p> <p>I assume some of what I&rsquo;ve written is incorrect as I&rsquo;m not an expert on v8 internals, but I do really love getting to the bottom of edge-case performance issues.</p> <h1 id=protocol>Protocol</h1> <p>The <a href=https://github.com/socketio/engine.io-protocol>engine.io protocol</a> allows bi-directional communication between a server and client, abstracting away the actual transport. The transport can be WebSockets, but if that isn&rsquo;t supported then another transport such as HTTP long polling is possible.</p> <p>When the WebSocket transport is used, packets are encapsulated by the engine.io protocol. First there is a number specifying the <a href=https://github.com/socketio/engine.io-protocol#packet>packet type</a>. For instance, ping packets starting with <code>2</code> are sent as WebSocket data even though WebSocket has its own heartbeat mechanism. Sending the JSON data <code>{"a": 123}</code> requires prefixing with a <code>4</code>. The socket.io protocol on top of that, if used, will add a <code>2</code> prefix meaning <a href=https://github.com/socketio/socket.io-protocol#packet>EVENT</a>, so a WebSocket listener will receive <code>"42{\"a\":123}"</code>.</p> <p>Using the long-polling transport, a payload containing multiple packets can be sent. In <a href=https://github.com/socketio/engine.io-protocol/tree/v3#payload>version 3 of the protocol</a>, the payload is encoded as:</p> <div class=highlight><pre><span></span>&lt;length1&gt;:&lt;packet1&gt;[&lt;length2&gt;:&lt;packet2&gt;[...]]
</pre></div> <p>e.g <code>6:42[{}]11:4abcdefghij1:2</code> contains 3 packets:</p> <ol> <li>Socket.io packet of length 6: Message (4), Event (2, <code>[{}]</code>)</li> <li>Packet of length 11: Message (4), Data (<code>abcdefghij</code>)</li> <li>Packet of length 1: Ping (2)</li> </ol> <p>With WebSockets, the 3 packets would be sent separately. With HTTP long polling, the payload would be POSTed to <code>http(s)://host/socket.io/?EIO=3&amp;transport=polling&amp;sid=$SESSIONID</code>.</p> <p>The denial-of-service bug lies in:</p> <ul> <li>Inefficient parsing of packets from payloads</li> <li>Maximum HTTP body size of 100MB</li> </ul> <h1 id=make-your-100mb-count>Make your 100MB count</h1> <p>You can send a payload containing 1e8 bytes to the server. That&rsquo;s quite a huge message, but how can we cause the server the most pain and suffering? The main methods are:</p> <ul> <li><strong>Many tiny packets</strong>: send 25,000,000 empty event packets <code>2:422:422:422:42</code></li> <li><strong>One giant int</strong>: send the largest possible packet with integer data <code>99999991:42222222222222222222...</code></li> <li><strong>Many heartbeats</strong>: send 33,333,333 ping packets <code>1:21:21:21:2...</code></li> </ul> <p>Loading the body string into memory automatically eats up 100MB as a starting point, but it gets a hell of a lot worse.</p> <h1 id=nodejs>Nodejs</h1> <p>With NodeJS, if the ping timeout (default 30s) is exceeded then the processing appears to be cancelled. Therefore, sending a payload which is so large it doesn&rsquo;t reach the memory exhausting step within the ping timeout will not kill the process. It will just waste CPU for 30 seconds. Sending a slightly smaller payload instead may cause the process to exit.</p> <h2 id=many-tiny-packets>Many tiny packets</h2> <p>The bug here is due to <a href=https://github.com/socketio/socket.io/commit/e60bd5a4da9173acba7553c9e631b79770a8c8be>this 2016 change</a>. As the parser reads packets from the payload, it doesn&rsquo;t emit the <code>socket.onpacket</code> event immediately. Instead it queues up a new closure with <code>process.nextTick</code>. Since the next tick of the event loop doesn&rsquo;t come until all packets have been parsed, memory usage blows up.</p> <p><figure><img alt="FixedQueue retaining many closures to be executed before the next tick" src=https://ℬ㏒.㎈ℓℯℛ.ⓧⓨℤ/socketio-engineio-dos/fixedqueue-showing-callback.png><figcaption>FixedQueue retaining many closures to be executed before the next tick</figcaption></figure></p> <p><code>process.nextTick</code> stores the closures in <a href=https://github.com/nodejs/node/blob/master/lib/internal/fixed_queue.js>FixedCircularBuffers inside a FixedQueue</a>. Each of these closures retains 200 bytes of heap memory (retained means that if this closure could be garbage collected, it would free this amount of heap memory). Not a lot per closure (no giant objects retained), but it adds up to ~5gb.</p> <p><figure><img alt="A huge number of closures. Closure code (96 bytes) + context (64 bytes) + packet (72 - 32 due to double counting) = 200 bytes" src=https://ℬ㏒.㎈ℓℯℛ.ⓧⓨℤ/socketio-engineio-dos/much-closure-context-previous.png><figcaption>A huge number of closures. Closure code (96 bytes) + context (64 bytes) + packet (72 - 32 due to double counting) = 200 bytes</figcaption></figure></p> <h2 id=one-giant-integer>One giant integer</h2> <p>This is best explained by looking at <a href=https://github.com/socketio/socket.io-parser/commit/dcb942d24db97162ad16a67c2a0cf30875342d55>my fix</a>.</p> <p>Luckily string concatenation in v8 doesn&rsquo;t create an entirely new string like in some languages where string builders are required. Instead, <code>a + b</code> becomes <code>ConsString { first = a, second = b }</code> pointing to the two smaller strings. There are even optimised versions <code>ConsOneByteString</code> and <code>ConsTwoByteString</code>.</p> <p>Sending the &ldquo;One giant int&rdquo; packet can cause OOM via building up many many <code>ConsOneByteString</code> objects (32 bytes each) due to concatenation: 99999989 <code>ConsOneByteString</code>s and then converting the massive integer to a <code>Number</code>.</p> <p><figure><img alt=ConsOneByteString src=https://ℬ㏒.㎈ℓℯℛ.ⓧⓨℤ/socketio-engineio-dos/consonebytestring.png><figcaption>ConsOneByteString</figcaption></figure></p> <div class=highlight><pre><span></span>==== JS stack trace =========================================

    0: ExitFrame [pc: 0x13c5b79]
Security context: 0x152fe7b808d1 &lt;JSObject&gt;
    1: decodeString [0x2dd385fb5d1] [/node_modules/socket.io-parser/index.js:~276] [pc=0xf59746881be](this=0x175d34c42b69 &lt;JSGlobal Object&gt;,0x14eccff10fe1 &lt;Very long string[69999990]&gt;)
    2: add [0x31fc2693da29] [/node_modules/socket.io-parser/index.js:242] [bytecode=0xa7ed6554889 offset=11](this=0x0a2881be5069 &lt;Decoder map = 0x3ceaa8bf48c9&gt;,0x14eccff10fe1 &lt;Very...

FATAL ERROR: Ineffective mark-compacts near heap limit Allocation failed - JavaScript heap out of memory
 1: 0xa09830 node::Abort() [node]
 2: 0xa09c55 node::OnFatalError(char const*, char const*) [node]
 3: 0xb7d71e v8::Utils::ReportOOMFailure(v8::internal::Isolate*, char const*, bool) [node]
 4: 0xb7da99 v8::internal::V8::FatalProcessOutOfMemory(v8::internal::Isolate*, char const*, bool) [node]
 5: 0xd2a1f5  [node]
 6: 0xd2a886 v8::internal::Heap::RecomputeLimits(v8::internal::GarbageCollector) [node]
 7: 0xd37105 v8::internal::Heap::PerformGarbageCollection(v8::internal::GarbageCollector, v8::GCCallbackFlags) [node]
 8: 0xd37fb5 v8::internal::Heap::CollectGarbage(v8::internal::AllocationSpace, v8::internal::GarbageCollectionReason, v8::GCCallbackFlags) [node]
 9: 0xd3965f v8::internal::Heap::HandleGCRequest() [node]
10: 0xce8395 v8::internal::StackGuard::HandleInterrupts() [node]
11: 0x1042cb6 v8::internal::Runtime_StackGuard(int, unsigned long*, v8::internal::Isolate*) [node]
12: 0x13c5b79  [node]
</pre></div> <h2 id=many-heartbeats>Many heartbeats</h2> <p>This causes OOM as many pongs are created to reply to all the pings.</p> <h1 id=python>Python</h1> <p>With eventlet, a single payload can DoS the entire server until processing completes due to the absence of <code>eventlet.sleep</code> calls. Without eventlet, the non-production server remains responsive until the thread pool is exhausted, so requires more than 1 concurrent request.</p> <h2 id=many-tiny-packets-special>Many tiny packets (special)</h2> <p>Payload: <code>2:4¼2:4¼2:4¼2:4¼2:4¼2:4¼...</code></p> <p>When non-ascii characters are present in the payload,</p> <div class=highlight><pre><span></span><span class=n>encoded_payload</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=s1>&#39;utf-8&#39;</span><span class=p>,</span> <span class=n>errors</span><span class=o>=</span><span class=s1>&#39;ignore&#39;</span><span class=p>)</span>
</pre></div> <p>is much slower:</p> <div class=highlight><pre><span></span><span class=nb>export</span> <span class=nv>N</span><span class=o>=</span><span class=m>100000</span><span class=p>;</span> python -m timeit -s <span class=s2>&quot;x=b&#39;2:42&#39; * </span><span class=nv>$N</span><span class=s2>&quot;</span> <span class=s2>&quot;x.decode(&#39;utf-8&#39;, errors=&#39;ignore&#39;)&quot;</span><span class=p>;</span> python -m timeit -s <span class=s2>&quot;x=b&#39;2:4\xbc&#39; * </span><span class=nv>$N</span><span class=s2>&quot;</span> <span class=s2>&quot;x.decode(&#39;utf-8&#39;, errors=&#39;ignore&#39;)&quot;</span><span class=p>;</span>
<span class=m>10000</span> loops, best of <span class=m>3</span>: <span class=m>37</span>.6 usec per loop
<span class=m>10</span> loops, best of <span class=m>3</span>: <span class=m>29</span>.3 msec per loop

<span class=nb>export</span> <span class=nv>N</span><span class=o>=</span><span class=m>10000000</span><span class=p>;</span> python -m timeit -s <span class=s2>&quot;x=b&#39;2:42&#39; * </span><span class=nv>$N</span><span class=s2>&quot;</span> <span class=s2>&quot;x.decode(&#39;utf-8&#39;, errors=&#39;ignore&#39;)&quot;</span><span class=p>;</span> python -m timeit -s <span class=s2>&quot;x=b&#39;2:4\xbc&#39; * </span><span class=nv>$N</span><span class=s2>&quot;</span> <span class=s2>&quot;x.decode(&#39;utf-8&#39;, errors=&#39;ignore&#39;)&quot;</span><span class=p>;</span>
<span class=m>100</span> loops, best of <span class=m>3</span>: <span class=m>9</span>.08 msec per loop
<span class=m>10</span> loops, best of <span class=m>3</span>: <span class=m>2</span>.95 sec per loop
</pre></div> <p>As engineio reads a packet, it decodes the entire remaining payload and then advances the length of the packet. So for an N-packet payload, the decode function is applied to:</p> <ul> <li>(string of N packets)</li> <li>(string of N-1 packets)</li> <li>(string of N-2 packets)</li> </ul> <p>so slowing down the decoding makes the DoS much more potent as it&rsquo;s O(n<sup>2</sup>)!</p> <p>This was <a href=https://github.com/miguelgrinberg/python-engineio/commit/64a34fc1550458ded57014301d5f9e97534f0843#diff-a2b90c63f58ef76954869020513f8e9bL72>fixed by the maintainer</a>.</p> <h2 id=all-others>All others</h2> <p>The python code seems to generally run slower than the nodejs code. Large payloads cause DoS primarily by wasting CPU time since python doesn&rsquo;t have a max heap size in the same way as v8. One giant int is slow as <code>int("2" * int(1e7))</code> is incredibly slow in python, perhaps because it allows Unicode digits like ٣ as well.</p> <h1 id=exploit>Exploit</h1> <p>I made a repo <a href=https://github.com/bcaller/kill-engine-io><i class="fa fa-github"></i> bcaller/kill-engine-io</a> containing test servers and code to trigger the DoS. Enjoy.</p> <p>Servers with a lower max HTTP body size are less vulnerable. In fact, the default has been lowered in newer versions.</p> </div> <div class=tags>Tags: <ul><li><a href=https://ℬ㏒.㎈ℓℯℛ.ⓧⓨℤ/tag/security>security</a></li><li><a href=https://ℬ㏒.㎈ℓℯℛ.ⓧⓨℤ/tag/poc>poc</a></li><li><a href=https://ℬ㏒.㎈ℓℯℛ.ⓧⓨℤ/tag/research>research</a></li><li><a href=https://ℬ㏒.㎈ℓℯℛ.ⓧⓨℤ/tag/nodejs>nodejs</a></li><li><a href=https://ℬ㏒.㎈ℓℯℛ.ⓧⓨℤ/tag/python>python</a></li></ul></div> </article> </main> <footer> <br><a href=https://ℬ㏒.㎈ℓℯℛ.ⓧⓨℤ>[blog by caller]</a> Correspondence welcome at ℬ㏒ {@} ㎈ℓℯℛ.ⓧⓨℤ </footer> </body> </html>